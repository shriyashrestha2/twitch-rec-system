{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitch Recommendation System\n",
    "\n",
    "Group Members: Ritwika Das, Nideesh Bharath Kumar, Shriya Shrestha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install websocket-client requests pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import threading\n",
    "import requests\n",
    "from pyngrok import conf, ngrok\n",
    "import websocket\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import glob\n",
    "from difflib import get_close_matches\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitch API Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client ID and Client Secret Keys to access Twitch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'gqvdji473lk1d7ka471jn9db93qjr9'\n",
    "CLIENT_SECRET = 'd2365e4zr946878dxy3q49wp4r78vj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Oauth Token from Twitch API to allow authorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_twitch_token():\n",
    "    url = 'https://id.twitch.tv/oauth2/token'\n",
    "    params = {\n",
    "        'client_id': CLIENT_ID,\n",
    "        'client_secret': CLIENT_SECRET,\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    res = requests.post(url, params=params)\n",
    "    res.raise_for_status()\n",
    "    return res.json()['access_token']\n",
    "\n",
    "access_token = get_twitch_token()\n",
    "\n",
    "HEADERS = {\n",
    "    'Client-ID': CLIENT_ID,\n",
    "    'Authorization': f'Bearer {access_token}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets a list of the live streamers currently streaming on Twitch, and returns the list as a .json file with the streamer usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_live_streamers(limit):\n",
    "    url = 'https://api.twitch.tv/helix/streams'\n",
    "    params = {'first': limit}\n",
    "    res = requests.get(url, headers=HEADERS, params=params)\n",
    "    res.raise_for_status()\n",
    "    data = res.json()['data']\n",
    "    return [stream['user_name'].lower() for stream in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleans the messages received from Twitch, by removing any links, characters, and making all words lowercase to simplify training of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(msg):\n",
    "    msg = re.sub(r\"http\\S+\", \"\", msg)\n",
    "    msg = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", msg)\n",
    "    return msg.lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entire class that has all functions for getting a message, cleaning it, storing it into a json based on the streamer, and placing it all in a folder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitchChatCollector:\n",
    "    def __init__(self, streamer):\n",
    "        self.streamer = streamer\n",
    "        self.messages = []\n",
    "        self.ws = None\n",
    "\n",
    "    def on_message(self, ws, message):\n",
    "        if \"PRIVMSG\" in message:\n",
    "            try:\n",
    "                parts = message.split(\":\", 2)\n",
    "                if len(parts) > 2:\n",
    "                    raw_msg = parts[2]\n",
    "                    cleaned = clean_message(raw_msg)\n",
    "                    self.messages.append(cleaned)\n",
    "                    print(f\"[{self.streamer}] {cleaned}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing message: {e}\")\n",
    "\n",
    "    # Opens the Twitch account to recieve the data from Twitch API\n",
    "    def on_open(self, ws):\n",
    "        ws.send(\"PASS oauth:tospw7iiv95sk1rqg5092rndixx70n\")\n",
    "        ws.send(\"NICK data_science_project\")\n",
    "        ws.send(f\"JOIN #{self.streamer}\")\n",
    "\n",
    "    # If an error occurs while collecting streamer information\n",
    "    def on_error(self, ws, error):\n",
    "        print(f\"[{self.streamer}] WebSocket error: {error}\")\n",
    "\n",
    "    # Closing the collecting data connection\n",
    "    def on_close(self, ws, code, msg):\n",
    "        print(f\"[{self.streamer}] Closed connection\")\n",
    "\n",
    "    # Start recieving any chat data from Twitch API\n",
    "    def start(self):\n",
    "        self.ws = websocket.WebSocketApp(\n",
    "            \"wss://irc-ws.chat.twitch.tv:443\",\n",
    "            on_open=self.on_open,\n",
    "            on_message=self.on_message,\n",
    "            on_error=self.on_error,\n",
    "            on_close=self.on_close\n",
    "        )\n",
    "        self.thread = threading.Thread(target=self.ws.run_forever)\n",
    "        self.thread.start()\n",
    "\n",
    "\n",
    "    # Stop recieving any chat data from Twitch API\n",
    "    def stop(self):\n",
    "        if self.ws:\n",
    "            self.ws.close()\n",
    "            self.thread.join()\n",
    "\n",
    "    # Saving all the chat data recieved from each streamer into .json files and storing them in a folder path\n",
    "    def save_messages(self, folder_path=\"twitch_chat_logs\"):\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        filename = f\"{self.streamer}_chat_{datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.messages, f, indent=2)\n",
    "\n",
    "        print(f\"Saved {len(self.messages)} messages for {self.streamer} to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection of Chat Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function that prints out all the chat data it is receiving from Twitch API and then displaying the total amount of chats from each streamer after collecting for about 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    folder_path = \"twitch_chat_logs\"\n",
    "    streamers = get_live_streamers(limit=20)\n",
    "    collectors = [TwitchChatCollector(s) for s in streamers]\n",
    "\n",
    "    print(f\"Starting chat collection for: {streamers}\")\n",
    "\n",
    "    for collector in collectors:\n",
    "        collector.start()\n",
    "\n",
    "    # Collect chat data for 60 seconds\n",
    "    time.sleep(60)\n",
    "\n",
    "    for collector in collectors:\n",
    "        collector.stop()\n",
    "        collector.save_messages(folder_path=folder_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting JSON to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are example user based input templates made for the model to determine what streamer would be a best fit for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_templates = [\n",
    "    \"I'm looking for a calm and cozy community.\",\n",
    "    \"I want an energetic and funny chat with lots of memes.\",\n",
    "    \"I prefer strategic talk and respectful discussion.\",\n",
    "    \"I like hype moments and esports energy.\",\n",
    "    \"I'm into chill vibes and friendly interactions.\",\n",
    "    \"I want chaotic and spammy but hilarious chat.\",\n",
    "    \"Looking for streamer with a welcoming and kind chat.\",\n",
    "    \"I want a toxic but entertaining and argumentative chat.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates training samples from the .json files of chat data we created right before this. Therefore we can train the model on this created dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects a random message from the list of chats in the .json file\n",
    "def sample_messages(messages, num=20):\n",
    "    return \"\\n\".join(random.sample(messages, min(num, len(messages))))\n",
    "\n",
    "# Generates a dataset by going through each .json file in the folder path and seperating information based on prompts, chats and responses\n",
    "def generate_dataset(chat_folder):\n",
    "    dataset = []\n",
    "    for file in os.listdir(chat_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(chat_folder, file), 'r') as f:\n",
    "                messages = json.load(f)\n",
    "                streamer = file.replace(\".json\", \"\").split(\"_chat_\")[0]\n",
    "                for _ in range(5):\n",
    "                    entry = {\n",
    "                        \"prompt\": random.choice(preference_templates),\n",
    "                        \"chats\": sample_messages(messages, num=20),\n",
    "                        \"response\": streamer\n",
    "                    }\n",
    "                    dataset.append(entry)\n",
    "    return dataset\n",
    "\n",
    "dataset = generate_dataset(\"twitch_chat_logs\")\n",
    "with open(\"chat_recommendation_dataset.json\", \"w\") as f:\n",
    "    for item in dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Created {len(dataset)} training samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the dataset created and checking the information and display some of the data to verify it was done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chat_recommendation_dataset.json\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n",
    "print(df.describe(include='all'))\n",
    "print(df['response'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, we were using DistilGPT2 as our LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "dataset = load_dataset('json', data_files='chat_recommendation_dataset.json', split='train')\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def format_for_lm(example):\n",
    "    prompt = f\"User: {example['prompt']}\\nChat:\\n{example['chats']}\\nRecommend:\"\n",
    "    target = f\" {example['response']}\"\n",
    "    example['text'] = prompt + target\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_for_lm)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llm_chat_recommender\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the training of the model, so that we do not have to keep training the model over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./llm_chat_recommender_final\")\n",
    "tokenizer.save_pretrained(\"./llm_chat_recommender_final\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llm_chat_recommender_final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llm_chat_recommender_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chat_log(file_path, max_lines=20):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return \"\\n\".join([line.strip() for line in lines[-max_lines:]])\n",
    "\n",
    "log_files = glob.glob(\"twitch_chat_logs/*.json\")\n",
    "latest_log = max(log_files, key=os.path.getctime)\n",
    "chat_text = load_chat_log(latest_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a reccomendation based on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendation(user_input, chat_logs, model, tokenizer, valid_streamers):\n",
    "    prompt = f\"\"\"User: {user_input}\n",
    "Chat:\\n{chat_logs}\n",
    "Recommend:\"\"\"\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the output using the model\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.6,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "    # Remove the prompt part if the model repeats it\n",
    "    predicted = decoded.split(\"Recommend:\")[-1].strip() if \"Recommend:\" in decoded else decoded\n",
    "\n",
    "    print(\"decoded\" + str(decoded))\n",
    "\n",
    "    # Debug print statements for raw output and predicted part\n",
    "    print(\"RAW MODEL OUTPUT:\", decoded)\n",
    "    print(\"PREDICTED PART ONLY:\", predicted)\n",
    "\n",
    "    # First, try direct containment to match a streamer name\n",
    "    for streamer in valid_streamers:\n",
    "        if streamer.lower() in predicted.lower():\n",
    "            return streamer\n",
    "\n",
    "    # If no direct match, use fuzzy matching to find the closest streamer\n",
    "    matches = get_close_matches(predicted.lower(), valid_streamers, n=1, cutoff=0.2)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    # If no match is found, return the \"Unknown\" message with the model's output\n",
    "    return f\"Unknown â€” model output: {predicted}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    valid_streamers = get_live_streamers(limit=5)\n",
    "    print(\"Valid streamers:\", valid_streamers)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching live streamers: {e}\")\n",
    "    valid_streamers = []\n",
    "\n",
    "user_input = \"I want an energetic and funny chat with lots of memes.\"\n",
    "streamer_key = generate_recommendation(user_input, chat_text, model, tokenizer, valid_streamers)\n",
    "\n",
    "print(\"Recommended Streamer:\", streamer_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing that our previous model was not always outputting a valid streamer, we realized that using fuzzy matching was not an optimal way to train based off of this data. Therefore, we decided to switch to a different model all-MiniLM-L6-v2. This model allowed better classification and improved in detecting a proper streamer based on the user prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want an energetic and funny chat with lots of memes.\"\n",
    "llm  = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def tidy(chat: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", chat.strip()).lower()\n",
    "\n",
    "body = [prompt]\n",
    "files  = []\n",
    "\n",
    "for path in glob.glob(\"twitch_chat_logs/*.json\"):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        doc = \" \".join(re.sub(r\"\\s+\", \" \", s.strip()).lower() for s in json.load(f) if isinstance(s, str))\n",
    "    body.append(doc)\n",
    "    files.append(os.path.basename(path))\n",
    "\n",
    "chat_embed = llm.encode(body, convert_to_tensor=True)\n",
    "scores = util.cos_sim(chat_embed[0], chat_embed[1:]).cpu().numpy().flatten()\n",
    "\n",
    "rec = files[scores.argmax()]\n",
    "print(f\"Streamer Rec: {rec.split('_')[0]}\")\n",
    "\n",
    "print(\"\\nTop 10 Rec:\")\n",
    "for i in (-scores).argsort()[:10]:\n",
    "    print(f\"{files[i]}  {scores[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows all the top similarity scores for streamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idx = np.argsort(-scores)[:10]\n",
    "top_files = [files[i].split('_')[0] for i in top_idx]\n",
    "top_scores = scores[top_idx]\n",
    "\n",
    "print(\"Top 10 Recommendations:\")\n",
    "for name, sc in zip(top_files, top_scores):\n",
    "    print(f\"{name}: {sc:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(top_files, top_scores)\n",
    "plt.xlabel(\"Streamer\")\n",
    "plt.ylabel(\"Cosine Similarity Score\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top 10 Streamer Recommendations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the spread of all the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(scores, bins=20)\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Count of Logs\")\n",
    "plt.title(\"Histogram of All Similarity Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the falloff in similarity scores related to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.arange(1, len(scores)+1)\n",
    "sorted_scores = np.sort(scores)[::-1]\n",
    "plt.figure()\n",
    "plt.scatter(ranks, sorted_scores)\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.title(\"Ranked Similarity Scores\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
